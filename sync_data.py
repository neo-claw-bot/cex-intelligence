#!/usr/bin/env python3
"""
æ•°æ®åŒæ­¥è„šæœ¬
å°†é‡‡é›†çš„æ•°æ®åŒæ­¥åˆ°ç½‘ç«™ç›®å½•
"""

import json
import shutil
from datetime import datetime
from pathlib import Path

def sync_data():
    """åŒæ­¥æ•°æ®åˆ°ç½‘ç«™ç›®å½•"""
    
    # æºæ•°æ®ç›®å½• (æ ¹ç›®å½•)
    source_dir = Path(__file__).parent / "data"
    
    # ç›®æ ‡ç›®å½• (webåº”ç”¨)
    web_data_dir = Path(__file__).parent / "web" / "data" / "intelligence"
    web_data_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰¾åˆ°æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    files = sorted(source_dir.glob("daily_*.json"))
    if not files:
        print("âŒ æ— æ•°æ®æ–‡ä»¶")
        return False
    
    latest = files[-1]
    print(f"ğŸ“‚ è¯»å–æºæ•°æ®: {latest}")
    
    # è¯»å–å¹¶è½¬æ¢æ ¼å¼
    with open(latest, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç”Ÿæˆæ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å (YYYY-MM-DD.json)
    today = datetime.now().strftime("%Y-%m-%d")
    target_file = web_data_dir / f"{today}.json"
    
    # è½¬æ¢æ•°æ®æ ¼å¼ä»¥é€‚åº”ç½‘ç«™æ¨¡æ¿
    exchanges_list = []
    for e in data.get("exchanges", []):
        # æ„å»ºè­¦æŠ¥åˆ—è¡¨
        alerts = []
        if e.get("fintelegram_reports"):
            for report in e.get("fintelegram_reports"):
                alerts.append({
                    "type": "fintelegram",
                    "title": report,
                    "severity": "high",
                    "description": report,
                    "source": "FinTelegram"
                })
        
        exchange_data = {
            "name": e.get("exchange"),
            "status": "warning" if e.get("alert_level") in ["high", "critical"] else "normal",
            "severity": e.get("alert_level", "none"),
            "alerts_count": len(alerts),
            "alerts": alerts,
            "twitter_sentiment": "neutral",
            "news_count": len(e.get("web_articles", [])),
            "sources": ["FinTelegram"] if alerts else []
        }
        exchanges_list.append(exchange_data)
    
    # æ„å»ºå…³é”®è­¦æŠ¥åˆ—è¡¨
    key_alerts_list = []
    for alert_text in data.get("key_alerts", []):
        key_alerts_list.append({
            "exchange": "MEXC" if "MEXC" in alert_text else "Unknown",
            "severity": "high",
            "title": alert_text,
            "description": alert_text,
            "source": "FinTelegram",
            "url": "https://fintelegram.com"
        })
    
    # è½¬æ¢æ•°æ®æ ¼å¼ä»¥é€‚åº”ç½‘ç«™
    web_data = {
        "date": today,
        "timestamp": data.get("timestamp", ""),
        "collected_at": data.get("timestamp", ""),  # æ¨¡æ¿ä½¿ç”¨è¿™ä¸ªå­—æ®µ
        "summary": {
            "total_exchanges": len(data.get("exchanges", [])),
            "alerted_exchanges": len([e for e in data.get("exchanges", []) if e.get("alert_level") != "none"]),
            "critical_alerts": len([e for e in data.get("exchanges", []) if e.get("alert_level") == "critical"]),
            "high_alerts": len([e for e in data.get("exchanges", []) if e.get("alert_level") == "high"])
        },
        "exchanges": data.get("exchanges", []),
        "exchanges_list": exchanges_list,  # æ¨¡æ¿ä½¿ç”¨è¿™ä¸ªæ ¼å¼
        "key_alerts": key_alerts_list,  # è½¬æ¢åçš„å…³é”®è­¦æŠ¥
        "alerts": key_alerts_list  # dashboardä½¿ç”¨
    }
    
    # ä¿å­˜åˆ°ç½‘ç«™ç›®å½•
    with open(target_file, 'w', encoding='utf-8') as f:
        json.dump(web_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²åŒæ­¥: {latest} â†’ {target_file}")
    
    # åŒæ—¶å¤åˆ¶åˆ° site/ ç›®å½•ç”¨äºé™æ€è®¿é—®
    site_data_dir = Path(__file__).parent / "site"
    site_data_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆç½‘ç«™ä¸“ç”¨æ•°æ®æ–‡ä»¶
    with open(site_data_dir / "latest.json", 'w', encoding='utf-8') as f:
        json.dump(web_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… é™æ€æ•°æ®å·²æ›´æ–°: site/latest.json")
    
    return True

if __name__ == "__main__":
    sync_data()
