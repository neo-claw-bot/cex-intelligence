#!/usr/bin/env python3
"""
CEX æ¯æ—¥ç®€æŠ¥ç”Ÿæˆå™¨ - ä¸­æ–‡ç‰ˆï¼ˆåˆ†æ‰¹é‡‡é›†ç‰ˆï¼‰
ç”¨äºå®šæ—¶ä»»åŠ¡ç”Ÿæˆæ¯æ—¥ç®€æŠ¥ï¼ˆä¸­æ–‡+å‡†ç¡®URLå¼•ç”¨ï¼‰
æ¯å¤©é‡‡é›†æ‰€æœ‰23ä¸ªäº¤æ˜“æ‰€çš„æœ€æ–°æƒ…æŠ¥
"""

import os
import json
import subprocess
from datetime import datetime
from pathlib import Path

def call_grok(prompt: str, tools: list, timeout: int = 120) -> dict:
    """è°ƒç”¨ Grok API"""
    api_key = os.getenv("XAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½® XAI_API_KEY ç¯å¢ƒå˜é‡")
        return {"error": "Missing API key"}
    
    data = {
        "model": "grok-4-1-fast-reasoning",
        "input": [{"role": "user", "content": prompt}],
        "tools": tools
    }
    
    curl_cmd = [
        "curl", "-s", "--max-time", str(timeout),
        "https://api.x.ai/v1/responses",
        "-H", "Content-Type: application/json",
        "-H", f"Authorization: Bearer {api_key}",
        "-d", json.dumps(data, ensure_ascii=False)
    ]
    
    try:
        result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=timeout+10)
        if result.returncode != 0:
            print(f"âš ï¸ curl é”™è¯¯: {result.stderr}")
            return {"error": result.stderr}
        return json.loads(result.stdout)
    except Exception as e:
        print(f"âš ï¸ è¯·æ±‚é”™è¯¯: {e}")
        return {"error": str(e)}

def extract_text(response: dict) -> str:
    """æå–å“åº”æ–‡æœ¬"""
    try:
        for item in response.get("output", []):
            if item.get("type") == "message" or item.get("role") == "assistant":
                for content in item.get("content", []):
                    if content.get("type") in ["text", "output_text"]:
                        return content.get("text", "")
    except:
        pass
    return ""

def is_generic_url(url: str) -> bool:
    """æ£€æŸ¥ URL æ˜¯å¦ä¸ºé€šç”¨é“¾æ¥ï¼ˆå®˜ç½‘é¦–é¡µç­‰ï¼‰"""
    if not url:
        return True
    
    generic_patterns = [
        "twitter.com/home", "twitter.com", "x.com",
        "kucoin.com", "binance.com", "coinbase.com",
        "bitget.com", "kraken.com", "okx.com", "bybit.com",
        "mexc.com", "gate.io", "htx.com", "crypto.com",
        "lbank.com", "upbit.com", "whitebit.com", "deribit.com",
        "fma.gv.at", "fintelegram.com",
        "coindesk.com", "cointelegraph.com"
    ]
    
    url_lower = url.lower().rstrip('/')
    for pattern in generic_patterns:
        if url_lower in [f"https://{pattern}", f"http://{pattern}", 
                        f"https://www.{pattern}", f"http://www.{pattern}"]:
            return True
    return False

def collect_exchange_batch(batch: list, batch_num: int, total_batches: int) -> dict:
    """é‡‡é›†ä¸€æ‰¹äº¤æ˜“æ‰€çš„æƒ…æŠ¥"""
    print(f"\nğŸ” [{batch_num}/{total_batches}] é‡‡é›†: {', '.join(batch)}")
    
    prompt = f"""æœç´¢ä»¥ä¸‹äº¤æ˜“æ‰€æœ€è¿‘24-48å°æ—¶çš„æƒ…æŠ¥ï¼ˆç”¨ä¸­æ–‡å›å¤ï¼‰ï¼š

äº¤æ˜“æ‰€: {', '.join(batch)}

æœç´¢ï¼šå®‰å…¨äº‹ä»¶ã€æç°é—®é¢˜ã€ç›‘ç®¡è¡ŒåŠ¨ã€æœåŠ¡ä¸­æ–­ã€è¯ˆéª—è­¦å‘Šã€é‡å¤§å…¬å‘Š

è¿”å›JSONï¼š
{{
  "alerts": [
    {{
      "exchange": "äº¤æ˜“æ‰€å",
      "severity": "critical|high|medium|low",
      "title": "ä¸­æ–‡æ ‡é¢˜",
      "description": "ä¸­æ–‡æè¿°",
      "url": "å…·ä½“é“¾æ¥æˆ–ç©º",
      "source_name": "æ¥æº",
      "tags": ["twitter","news","regulatory","security","user_report"]
    }}
  ],
  "exchange_status": {{
    "äº¤æ˜“æ‰€å": {{"status": "normal|warning|critical", "notes": "è¯´æ˜", "url": "é“¾æ¥"}}
  }},
  "sources": [{{"name": "æ¥æº", "url": "é“¾æ¥"}}]
}}

è§„åˆ™ï¼š
1. æ— äº‹ä»¶è¿”å›ç©ºæ•°ç»„
2. URLè¦å…·ä½“æ–‡ç« ï¼Œä¸è¦å®˜ç½‘
3. å¿…é¡»ç”¨ä¸­æ–‡
4. æ¯ä¸ªäº¤æ˜“æ‰€æœ‰ç‹¬ç«‹çŠ¶æ€"""
    
    response = call_grok(prompt, [{"type": "x_search"}, {"type": "web_search"}], timeout=100)
    text = extract_text(response)
    
    try:
        data = json.loads(text) if text else {}
        alerts = data.get("alerts", [])
        print(f"   âœ… å‘ç° {len(alerts)} æ¡è­¦æŠ¥")
        return data
    except Exception as e:
        print(f"   âš ï¸ è§£æå¤±è´¥: {e}")
        return {}

def collect_daily_intel() -> dict:
    """é‡‡é›†æ¯æ—¥æƒ…æŠ¥ - åˆ†æ‰¹é‡‡é›†æ‰€æœ‰23ä¸ªäº¤æ˜“æ‰€"""
    exchanges = [
        "Binance", "MEXC", "Gate", "Bitget", "OKX", "HTX", "Bybit", "Coinbase",
        "CoinW", "BitMart", "Crypto.com", "DigiFinex", "LBank", "Upbit", "Toobit",
        "WEEX", "P2B", "XT.COM", "Tapbit", "Kraken",
        "KuCoin", "WhiteBIT", "Deribit"
    ]
    today = datetime.now().strftime("%Y-%m-%d")
    
    print("=" * 70)
    print(f"ğŸš€ CEX æ¯æ—¥æƒ…æŠ¥é‡‡é›†å¯åŠ¨ | {today}")
    print(f"ğŸ“Š ç›®æ ‡: {len(exchanges)} ä¸ªäº¤æ˜“æ‰€")
    print("=" * 70)
    
    # åˆ†æ‰¹é‡‡é›†ï¼ˆæ¯æ‰¹5-6ä¸ªï¼Œé¿å…è¶…æ—¶ï¼‰
    batch_size = 6
    batches = [exchanges[i:i+batch_size] for i in range(0, len(exchanges), batch_size)]
    
    all_alerts = []
    all_exchange_status = {}
    all_sources = []
    
    for i, batch in enumerate(batches, 1):
        data = collect_exchange_batch(batch, i, len(batches))
        
        # åˆå¹¶è­¦æŠ¥
        if data.get("alerts"):
            for alert in data["alerts"]:
                if is_generic_url(alert.get("url", "")):
                    alert["url"] = ""
                all_alerts.append(alert)
        
        # åˆå¹¶çŠ¶æ€
        if data.get("exchange_status"):
            for ex, info in data["exchange_status"].items():
                if is_generic_url(info.get("url", "")):
                    info["url"] = ""
                all_exchange_status[ex] = info
        
        # åˆå¹¶æ¥æº
        if data.get("sources"):
            all_sources.extend(data["sources"])
    
    # ç¡®ä¿æ‰€æœ‰äº¤æ˜“æ‰€æœ‰çŠ¶æ€è®°å½•
    for ex in exchanges:
        if ex not in all_exchange_status:
            all_exchange_status[ex] = {"status": "normal", "notes": "", "url": ""}
    
    # ç”Ÿæˆæ‘˜è¦
    summary = generate_summary(all_alerts)
    
    final_data = {
        "date": today,
        "collected_at": datetime.now().isoformat(),
        "summary": summary,
        "alerts": all_alerts,
        "exchange_status": all_exchange_status,
        "fintelegram_highlights": [],
        "sources": all_sources,
        "total_exchanges": len(exchanges),
        "total_batches": len(batches)
    }
    
    print("\n" + "=" * 70)
    print(f"âœ… é‡‡é›†å®Œæˆ")
    print(f"ğŸ“Š æ€»è®¡: {len(all_alerts)} æ¡æƒ…æŠ¥")
    print(f"ğŸ¢ è¦†ç›–: {len(all_exchange_status)} ä¸ªäº¤æ˜“æ‰€")
    print(f"ğŸ“ æ‘˜è¦: {summary[:60]}...")
    print("=" * 70)
    
    return final_data

def generate_summary(alerts: list) -> str:
    """æ ¹æ®è­¦æŠ¥ç”Ÿæˆæ‘˜è¦"""
    if not alerts:
        return "è¿‡å»24-48å°æ—¶å†…ï¼Œæ‰€æœ‰ç›‘æ§çš„23ä¸ªäº¤æ˜“æ‰€è¿è¥æ­£å¸¸ï¼Œæœªå‘ç°é‡å¤§å®‰å…¨äº‹ä»¶ã€ç›‘ç®¡è¡ŒåŠ¨æˆ–ç”¨æˆ·æŠ•è¯‰ã€‚"
    
    critical = len([a for a in alerts if a.get("severity") == "critical"])
    high = len([a for a in alerts if a.get("severity") == "high"])
    exchanges = list(set(a.get("exchange", "") for a in alerts))[:3]
    
    if critical > 0:
        return f"è¿‡å»24-48å°æ—¶å‘ç°{critical}èµ·ä¸¥é‡äº‹ä»¶ï¼Œæ¶‰åŠ{', '.join(exchanges)}ç­‰äº¤æ˜“æ‰€ï¼Œå»ºè®®ç«‹å³å…³æ³¨å¹¶é‡‡å–é˜²èŒƒæªæ–½ã€‚"
    elif high > 0:
        return f"è¿‡å»24-48å°æ—¶å‘ç°{high}èµ·é«˜é£é™©äº‹ä»¶ï¼Œæ¶‰åŠ{', '.join(exchanges)}ç­‰äº¤æ˜“æ‰€ï¼Œéœ€è¦å¯†åˆ‡å…³æ³¨åŠ¨æ€ã€‚"
    else:
        return f"è¿‡å»24-48å°æ—¶å‘ç°{len(alerts)}èµ·ä¸€èˆ¬æ€§äº‹ä»¶ï¼Œæ¶‰åŠ{', '.join(exchanges)}ç­‰äº¤æ˜“æ‰€ï¼Œæ•´ä½“é£é™©å¯æ§ã€‚"

def save_intel(data: dict):
    """ä¿å­˜æƒ…æŠ¥åˆ°æ–‡ä»¶"""
    # é¡¹ç›®ç›®å½•
    data_dir = Path("/Users/neo/.openclaw/workspace-cex-intelligence/data/intelligence")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # web ç›®å½•ï¼ˆç”¨äºéƒ¨ç½²ï¼‰
    web_data_dir = Path("/Users/neo/.openclaw/workspace-cex-intelligence/web/data/intelligence")
    web_data_dir.mkdir(parents=True, exist_ok=True)
    
    date = data['date']
    
    # ä¿å­˜åˆ°ä¸¤ä¸ªä½ç½®
    for dir_path in [data_dir, web_data_dir]:
        filepath = dir_path / f"{date}.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜: {filepath}")
    
    # åŒæ—¶ä¿å­˜ä¸ºæœ€æ–°ç®€æŠ¥
    briefing_file = Path("/Users/neo/.openclaw/workspace-cex-intelligence/data/last_briefing.txt")
    with open(briefing_file, 'w', encoding='utf-8') as f:
        f.write(format_discord_message(data))
    
    return filepath

def format_discord_message(data: dict) -> str:
    """æ ¼å¼åŒ–ä¸º Discord æ¶ˆæ¯"""
    lines = [f"## ğŸ¯ CEX æƒ…æŠ¥æ¯æ—¥ç®€æŠ¥\nğŸ“… {data['date']}\n"]
    
    alerts = data.get("alerts", [])
    critical = [a for a in alerts if a.get("severity") == "critical"]
    high = [a for a in alerts if a.get("severity") == "high"]
    
    if critical:
        lines.append("ğŸš¨ **ä¸¥é‡è­¦æŠ¥**")
        for a in critical[:2]:
            lines.append(f"ğŸ”´ **{a['exchange']}**: {a['title']}")
    
    if high:
        lines.append("\nâš ï¸ **é«˜é£é™©äº‹ä»¶**")
        for a in high[:3]:
            lines.append(f"ğŸŸ  **{a['exchange']}**: {a['title']}")
    
    lines.append("\nğŸ“Š **äº¤æ˜“æ‰€çŠ¶æ€æ¦‚è§ˆ**")
    for ex, info in list(data.get("exchange_status", {}).items())[:5]:
        emoji = {"normal": "ğŸŸ¢", "warning": "ğŸŸ¡", "critical": "ğŸ”´"}.get(info.get("status"), "âšª")
        notes = info.get("notes", "")[:30]
        lines.append(f"{emoji} **{ex}**: {notes if notes else 'æ­£å¸¸'}")
    
    lines.append(f"\nğŸ’¡ **æ‘˜è¦**: {data.get('summary', '')[:100]}...")
    lines.append("\nâ€”")
    lines.append("ğŸ”— æŸ¥çœ‹è¯¦æƒ…: https://cex-intelligence-production.up.railway.app")
    
    return "\n".join(lines)

def main():
    """ä¸»å…¥å£"""
    print("ğŸš€ CEX Intelligence - æ¯æ—¥æƒ…æŠ¥é‡‡é›†ç³»ç»Ÿ")
    print("ğŸ“ é‡‡é›†æ‰€æœ‰23ä¸ªäº¤æ˜“æ‰€çš„æœ€æ–°æƒ…æŠ¥\n")
    
    # é‡‡é›†æ•°æ®
    data = collect_daily_intel()
    
    # ä¿å­˜
    save_intel(data)
    
    print("\nâœ… å®Œæˆï¼æ•°æ®å·²ä¿å­˜å¹¶å‡†å¤‡å‘å¸ƒã€‚")
    print(f"ğŸ“Š å…±é‡‡é›† {len(data.get('alerts', []))} æ¡æƒ…æŠ¥")
    print(f"ğŸŒ ç½‘ç«™: https://cex-intelligence-production.up.railway.app")

if __name__ == "__main__":
    main()
